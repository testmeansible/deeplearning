{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a1c6cf",
   "metadata": {},
   "source": [
    "## Include all necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Libraries\n",
    "import os                                                                 # OS Library\n",
    "import pandas as pd                                                       # Pandas Library\n",
    "import numpy as np                                                        # Numpy Library\n",
    "from fastai.tabular.all import df_shrink                                  # Library to shrink \n",
    "from fastcore.parallel import *                                           # Library for parallel tasks\n",
    "import time                                                               # Time function library\n",
    "import matplotlib.pyplot as plt                                           # plot library\n",
    "import seaborn as sns                                                     # plot library\n",
    "# import memory_profiler, memory_                                         # memory usage library\n",
    "from memory_profiler import memory_usage\n",
    "from collections import Counter                                           # Counter library\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Libraries for feature engineering\n",
    "from sklearn.model_selection import train_test_split                      # Dataset split library\n",
    "# from sklearn.model_selection import GridSearchCV                          # Gridsearch library for hyper parameter tuning\n",
    "from sklearn.preprocessing import LabelEncoder                            # LabelEnoder\n",
    "# from sklearn.preprocessing import OneHotEncoder                           # Onehotencoding \n",
    "from sklearn.feature_selection import VarianceThreshold                   # For removing zero variance features\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler            # Apply Standarization and Normalization \n",
    "from imblearn.over_sampling import SMOTE, SMOTEN, SMOTENC                 # Over-sampling Library\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks        # Under-Sampling\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# ML Model Libraries\n",
    "# from sklearn.linear_model import LogisticRegression                       # Logistic Regression\n",
    "# from sklearn.tree import DecisionTreeClassifier                           # Decision Tree Classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier                       # Random forest Classifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier                           # AdaBoost Classifier\n",
    "# from sklearn.svm import SVC                                               # SVM Library\n",
    "# from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB   # naive base classifiers library\n",
    "\n",
    "# Performance measure Libraries for Trained model \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "# Deep learning Model Libraries\n",
    "# import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f526aa3",
   "metadata": {},
   "source": [
    "## Create a csv read function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvRead (sfilePath):\n",
    "    print(\"CSV Read function started........\")\n",
    "#     ind_dfs = [pd.read_csv(dsp) for dsp in filePathArray]\n",
    "    ind_dfs = pd.read_csv(sfilePath)\n",
    "    return ind_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a20f30",
   "metadata": {},
   "source": [
    "## Create a datatype downsizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataTypeDown(ind_dfs_read):\n",
    "    print(\"Datatype Downsizing function started........\")\n",
    "#     ind_dfs_down = parallel(df_shrink, ind_dfs_read, progress=True)\n",
    "    ind_dfs_down = df_shrink(ind_dfs_read)\n",
    "    return ind_dfs_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe7917",
   "metadata": {},
   "source": [
    "## List the files in our target directory we are interested to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = []\n",
    "individual_dfs = []\n",
    "for root, subdir, files in os.walk('D:/Datasets/CICDDoS2019/testing_03_11/'):\n",
    "# print(root)\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            fpath = os.path.join(root,file)\n",
    "            filePath.append(fpath)\n",
    "            print (fpath)\n",
    "            print(\"-----------------------------------------------------------------------\")\n",
    "#             print (f'Total no of files meet our criteria are: ', len(filePath))\n",
    "            if (len(filePath)):\n",
    "                ind_dfs_read = csvRead(fpath)\n",
    "#                 time.sleep(15)\n",
    "                print(\"starting downsizing.....\")\n",
    "#                 individual_dfs = [dataTypeDown(ind_dfs_read)]\n",
    "                individual_dfs.append(dataTypeDown(ind_dfs_read))\n",
    "                print(f'No. of Instances in the file are:',ind_dfs_read.shape)\n",
    "#                 time.sleep(5)\n",
    "print('**************************')\n",
    "print (f'Total no of files meet our criteria are: ', len(filePath))\n",
    "print(\"Dataset Read Task Completed!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ba3b7",
   "metadata": {},
   "source": [
    "## Replace space with \"_\" in dataset column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in individual_dfs:\n",
    "    # Remove leading and trailing spaces from column names\n",
    "#     cleaned_columns = [col.strip() for col in cols]\n",
    "#     print(cleaned_columns)\n",
    "    cols = i.columns.str.strip()\n",
    "#     cols = [col.strip() for col in cols]\n",
    "    cols = cols.map(lambda x: x.replace(' ', '_') )\n",
    "    i.columns = cols\n",
    "#     print(i.columns)\n",
    "print('Task Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6be513",
   "metadata": {},
   "source": [
    "## Dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279addf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual_dfs[0].info(verbose=True)\n",
    "individual_dfs[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8036c64",
   "metadata": {},
   "source": [
    "## Dataset split into X (input) and y (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_X = []\n",
    "individual_y = []\n",
    "count = 0\n",
    "for i in individual_dfs:\n",
    "#     Target (Output) values\n",
    "    temp_y = i.copy()\n",
    "    individual_y.append(i['Label'])\n",
    "    print(individual_y[count].shape)\n",
    "#     Input values\n",
    "    temp_X = i.copy()\n",
    "    temp_X = temp_X.drop('Label', axis=1)\n",
    "    individual_X.append(temp_X)\n",
    "    print(individual_X[count].shape)\n",
    "    count+=1\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddeda0",
   "metadata": {},
   "source": [
    "## Verify the split task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_X[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get detail overview of the dataset run this command\n",
    "# individual_X[0].info(verbose=True)\n",
    "individual_X[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474b8c0",
   "metadata": {},
   "source": [
    "## Further Processing on dataset by concatinating All files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e04829",
   "metadata": {},
   "source": [
    "## Concatinating dataset file into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16345ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X =  pd.concat(individual_X, ignore_index=True)\n",
    "all_y = pd.concat(individual_y, ignore_index=True)\n",
    "all_X.shape, all_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "# Create a count plot of the string labels\n",
    "ax = sns.countplot(x=all_y)\n",
    "# Annotate each bar with its count\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count Per Label in CIC-DDoS 2019 Dataset Day-1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b545ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value_count = all_y.value_counts()\n",
    "unique_value_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c49636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize distribution\n",
    "counter = Counter(all_y)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(all_y) * 100\n",
    "    print('Class = %-15s,     n = %-10d,        (%-8.3f%%)' % (k, v, per))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e7819",
   "metadata": {},
   "source": [
    "## Dataset verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688b288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_X.iloc[20:30].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca11320",
   "metadata": {},
   "source": [
    "## Label encoding performed on all_y dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(all_y)\n",
    "\n",
    "# Get the mapping of original labels to encoded values\n",
    "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "\n",
    "# Print the mapping\n",
    "print(\"Label Mapping:\")\n",
    "for label, encoded_value in label_mapping.items():\n",
    "    print(f\"{label}: {encoded_value}\")\n",
    "\n",
    "print (f\"Shape of the target values {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f292d",
   "metadata": {},
   "source": [
    "## Train - Test Split on all 77 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3 # number of training raws (70%-30%)\n",
    "random_state = 42 # random seed\n",
    "\n",
    "X_train_v0, X_test_v0, y_train_v0, y_test_v0 = train_test_split(all_X, y_encoded, \n",
    "                                                                test_size=test_size, \n",
    "                                                                shuffle=True,\n",
    "                                                                stratify=y_encoded, \n",
    "                                                                random_state=random_state)\n",
    "X_train_v0.shape, y_train_v0.shape, X_test_v0.shape, y_test_v0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d2383",
   "metadata": {},
   "source": [
    "## Dataset Creation with StandardScaler-Random Undersampler-SMOTE with 77 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776ecdb",
   "metadata": {},
   "source": [
    "## Calculate the minimum, mean, and median number of instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300feb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum, mean, and median number of instances per class\n",
    "class_distribution = pd.Series(y_train_v0).value_counts()\n",
    "min_instances = class_distribution.min()\n",
    "mean_instances = int(class_distribution.mean())\n",
    "median_instances = int(class_distribution.median())\n",
    "min_instances, mean_instances, median_instances\n",
    "# sampling_strategy = {cls: mean_instances for cls in class_distribution.index}\n",
    "# print(sampling_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, threshold, scaler):\n",
    "    print(f'The shape of X and y is ',X.shape, y.shape)\n",
    "    # Determine the class labels\n",
    "    class_label = pd.Series(y).unique()\n",
    "\n",
    "    print('------------------------------------------------')\n",
    "    print(f'The class encoded labels are ',class_label)\n",
    "    print (f'The threshold is ', threshold)\n",
    "    print (f'The scaler is ', scaler)\n",
    "    print('------------------------------------------------')\n",
    "    print(f'dataset creation process started for : {threshold} instances for each Class' )\n",
    "    \n",
    "    # Determine the classes that need under-sampling and SMOTE\n",
    "    under_sample_classes = [class_label for class_label, count in class_distribution.items() if count > threshold]\n",
    "    smote_classes = [class_label for class_label, count in class_distribution.items() if count <= threshold]\n",
    "\n",
    "    print(f'under sampling classes are: ', under_sample_classes)\n",
    "    print(f'SMOT Over sampling classes are: ', smote_classes)\n",
    "    \n",
    "    # Create a sampling strategy dictionary\n",
    "    rus_sampling_strategy = {}\n",
    "    smt_sampling_strategy = {}\n",
    "    \n",
    "    for class_label in under_sample_classes:\n",
    "        rus_sampling_strategy[class_label] = threshold\n",
    "    \n",
    "    for class_label in smote_classes:\n",
    "        smt_sampling_strategy [class_label]= threshold\n",
    "    \n",
    "#     transformer = ColumnTransformer([\n",
    "#         ('scale', StandardScaler(), X.columns)\n",
    "#     ])\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "#         ('transformer', transformer),\n",
    "        # ('scale', StandardScaler()),\n",
    "        ('scale', scaler), # Now getting the scaler to work, from function call\n",
    "        ('rus', RandomUnderSampler(sampling_strategy=rus_sampling_strategy)),\n",
    "        ('smote', SMOTE(sampling_strategy=smt_sampling_strategy))\n",
    "    ], verbose=True)\n",
    "\n",
    "    # Apply the pipeline to create a new dataset\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "#     pipeline.fit(X,y)\n",
    "    print(f'dataset creation process completed for : {threshold} instances for each Class' )\n",
    "    return X_resampled, y_resampled, pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fd0e6",
   "metadata": {},
   "source": [
    "## Dataset Creation with Standard-Scaler Random Undersampler and SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three datasets based on the thresholds\n",
    "# X_min_SS_77, y_min_SS_77, pipeline_min_SS_77 = create_datasets(X_train_v0, y_train_v0, min_instances, StandardScaler()) \n",
    "# X_mean_SS_77, y_mean_SS_77, pipeline_mean_SS_77 = create_datasets(X_train_v0, y_train_v0, mean_instances, StandardScaler()) \n",
    "X_median_SS_77, y_median_SS_77, pipeline_median_SS_77 = create_datasets(X_train_v0, y_train_v0, median_instances, StandardScaler()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b71181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_min_SS_77.shape, y_min_SS_77.shape, X_mean_SS_77.shape, y_mean_SS_77.shape, X_median_SS_77.shape, y_median_SS_77.shape\n",
    "X_median_SS_77.shape, y_median_SS_77.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize class distribution\n",
    "def plot_class_distribution(y, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=pd.Series(y).value_counts().index, y=pd.Series(y).value_counts().values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Number of Instances\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each dataset\n",
    "# plot_class_distribution(y_min_SS_77, \"Class Distribution (Min Instances)\")\n",
    "# plot_class_distribution(y_mean_SS_77, \"Class Distribution (mean Instances)\")\n",
    "plot_class_distribution(y_median_SS_77, \"Class Distribution (Median Instances)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acb02d",
   "metadata": {},
   "source": [
    "## X_test dataset scaling with pipeline scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_min_SS_77 = pipeline_min_SS_77.named_steps['scale'].transform(X_test_v0)\n",
    "# X_test_mean_SS_77 = pipeline_mean_SS_77.named_steps['scale'].transform(X_test_v0)\n",
    "X_test_median_SS_77 = pipeline_median_SS_77.named_steps['scale'].transform(X_test_v0)\n",
    "\n",
    "# X_test_min_SS_77.shape, X_test_mean_SS_77.shape, X_test_median_SS_77.shape\n",
    "X_test_median_SS_77.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cf20",
   "metadata": {},
   "source": [
    "### Create Validation Set from median-training set of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b46031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a validation set (20 of training data)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_median_SS_77, y_median_SS_77, test_size=0.2, random_state=42, stratify=y_median_SS_77)\n",
    "\n",
    "# Check dataset shapes\n",
    "print(f\"Training Data Shape: {X_train_final.shape}, {y_train_final.shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test Data Shape: {X_test_median_SS_77.shape}, {y_test_v0.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fa736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_final, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_final, dtype=torch.long)  # Classification task\n",
    "\n",
    "# Convert validation data to tensors\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)  # StandardScaler applied\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Convert test data to tensors\n",
    "X_test_tensor = torch.tensor(X_test_median_SS_77, dtype=torch.float32)  # StandardScaler applied\n",
    "y_test_tensor = torch.tensor(y_test_v0, dtype=torch.long)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Training Data Shape:\", X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(\"Validation Data Shape:\", X_val_tensor.shape, y_val_tensor.shape)\n",
    "print(\"Test Data Shape:\", X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adae06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef9987",
   "metadata": {},
   "source": [
    "### Create a DataLoader\n",
    "#### DataLoader helps with batching, shuffling, and parallel processing for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 64  # You can change this based on your system's memory\n",
    "# Automatically determine an optimal number of workers\n",
    "num_workers = min(4, torch.get_num_threads())  # Adjust based on your CPU\n",
    "print(\"Number of workers:\", num_workers)\n",
    "# Create DataLoader for training and validation data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False, num_workers=4, pin_memory=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8086e",
   "metadata": {},
   "source": [
    "### Verify DataLoader\n",
    "#### To check if the DataLoader works correctly, let's load a single batch and inspect the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41689a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a batch of data\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(\"Batch X Shape:\", X_batch.shape)\n",
    "print(\"Batch y Shape:\", y_batch.shape)\n",
    "\n",
    "Xtest_batch, ytest_batch = next(iter(test_loader))\n",
    "print(\"Batch xtest Shape:\", Xtest_batch.shape)\n",
    "print(\"Batch ytest Shape:\", ytest_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdede843",
   "metadata": {},
   "source": [
    "## Training and making prediction on resampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network model\n",
    "class DDoSClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DDoSClassifier, self).__init__()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # Input layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Dropout for regularization\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, num_classes)  # Output layer\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax activation for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)  # Apply softmax for classification\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Get input size (number of features) and number of classes\n",
    "input_size = X_train_tensor.shape[1]  # Number of features\n",
    "num_classes = len(set(y_train_tensor))  # Number of unique labels\n",
    "\n",
    "# Create the model instance\n",
    "model = DDoSClassifier(input_size, num_classes)\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def memory_usage():\n",
    "    \"\"\"Returns memory usage in MB.\"\"\"\n",
    "    return psutil.Process().memory_info().rss / 1024 ** 2  # Convert bytes to MB\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    print(f\" Training on device: {device}\")\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    start_time = time.time()  # Track training start time\n",
    "    mem_usage_before = memory_usage()  # Track memory usage before training\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Mixed Precision Training\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        # Use tqdm progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for X_batch, y_batch in progress_bar:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            correct += (y_pred.argmax(1) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "            avg_loss = train_loss / len(train_loader)\n",
    "            accuracy = correct / total\n",
    "            progress_bar.set_postfix(loss=avg_loss, acc=accuracy)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                correct += (y_pred.argmax(1) == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # Track time and memory usage after training\n",
    "    end_time = time.time()\n",
    "    mem_usage_after = memory_usage()\n",
    "\n",
    "    train_time = end_time - start_time\n",
    "    train_mem_used = mem_usage_after - mem_usage_before\n",
    "\n",
    "    print(f\"Training Completed! \")\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\"Memory Used: {train_mem_used:.2f} MB\")\n",
    "\n",
    "    return train_time, train_mem_used, model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_predictions(model, test_loader, device):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    start_time = time.time()\n",
    "    mem_usage_before = memory_usage()\n",
    "\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:  # Iterate using batch\n",
    "            X_batch, _ = batch  # Correctly unpack batch\n",
    "            # print(f\"Batch Type: {type(X_batch)}\")  # Debugging statement\n",
    "\n",
    "            X_batch = X_batch.to(device)  # Move to device\n",
    "            y_pred = model(X_batch)\n",
    "            y_pred_list.append(y_pred.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    mem_usage_after = memory_usage()\n",
    "\n",
    "    predict_time = end_time - start_time\n",
    "    mem_used = mem_usage_after - mem_usage_before\n",
    "\n",
    "    return predict_time, mem_used, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d109e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_report(y_test, y_pred, model, file_name, train_time, train_mem_used, predict_time, predict_mem_used):\n",
    "    \n",
    "    try:\n",
    "        # Convert inputs to NumPy arrays if they are tensors\n",
    "        if isinstance(y_test, torch.Tensor):\n",
    "            y_test = y_test.cpu().numpy()\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "        # Classification report & Confusion matrix\n",
    "        report = classification_report(y_test, y_pred, digits=5, output_dict=True)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "    \n",
    "        # Create directories\n",
    "        base_directory = 'DL_Model_Reports'\n",
    "        dir_name = 'MLP_pytorch_reports'\n",
    "        output_directory = os.path.join(base_directory, dir_name)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        output_file = os.path.join(output_directory, f'{file_name}_report.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "            report_df.to_excel(writer, sheet_name='Classification Report')\n",
    "            conf_matrix_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "            pd.DataFrame({'Training Time': [train_time]}).to_excel(writer, sheet_name='Training Time')\n",
    "            pd.DataFrame({'Training Memory Used': [train_mem_used]}).to_excel(writer, sheet_name='Training Memory Used')\n",
    "            pd.DataFrame({'Prediction Time': [predict_time]}).to_excel(writer, sheet_name='Prediction Time')\n",
    "            pd.DataFrame({'Prediction Memory Used': [predict_mem_used]}).to_excel(writer, sheet_name='Prediction Memory Used')\n",
    "\n",
    "            # Save model architecture\n",
    "            model_summary = [str(model)]\n",
    "        # model_summary.append(str(model))  # Save model details\n",
    "            pd.DataFrame({'Model Summary': model_summary}).to_excel(writer, sheet_name='Model Summary')\n",
    "\n",
    "        print(f\"Report saved successfully: {output_file}\")\n",
    "        return report, conf_matrix\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and prediction with median_dataset\n",
    "\n",
    "# Define Hyperparameters\n",
    "EPOCHS = 20\n",
    "# BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Define Loss Function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move model to GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train Model\n",
    "train_time, train_mem_used, trained_model, history = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, EPOCHS, device)\n",
    "\n",
    "print(f\"Training time: {train_time:.2f} seconds, Memory used: {train_mem_used:.2f} MB\")\n",
    "\n",
    "# Make Predictions\n",
    "predict_time, predict_mem_used, y_pred_test = make_predictions(\n",
    "    trained_model, test_loader, device)\n",
    "\n",
    "print(f\"Prediction time: {predict_time:.2f} seconds, Memory used: {predict_mem_used:.2f} MB\")\n",
    "\n",
    "# Extract true labels from test_loader\n",
    "y_true_test = np.concatenate([y_batch.numpy() for _, y_batch in test_loader], axis=0)\n",
    "\n",
    "# Generate Classification Report\n",
    "file_name = \"MLP_model_median_ss_77\"\n",
    "report, confusion_mat = make_report(y_true_test, y_pred_test, trained_model, file_name, \n",
    "                                    train_time, train_mem_used, predict_time, predict_mem_used)\n",
    "\n",
    "print(\"Model evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract history details\n",
    "train_loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "train_acc = history['train_acc']\n",
    "val_acc = history['val_acc']\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "\n",
    "# Convert numerical predictions back to original labels\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_v0)  # Convert y_test to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_median_ss_77)  # Convert predictions to original labels\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)  # Rotate labels if they are long\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec70270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd97dc46",
   "metadata": {},
   "source": [
    "## Dataset Creation with MinMax-Random Undersampler-SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three datasets based on the thresholds\n",
    "# X_min_mm, y_min_mm, pipeline_min_mm = create_datasets(X_train_v1, y_train_v1, min_instances, MinMaxScaler())\n",
    "# X_mean_mm_77, y_mean_mm_77, pipeline_mean_mm_77 = create_datasets(X_train_v0, y_train_v0, mean_instances, MinMaxScaler()) \n",
    "X_median_mm_77, y_median_mm_77, pipeline_median_mm_77 = create_datasets(X_train_v0, y_train_v0, median_instances, MinMaxScaler()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_min_SS_77.shape, y_min_SS_77.shape, X_mean_SS_77.shape, y_mean_SS_77.shape, X_median_SS_77.shape, y_median_SS_77.shape\n",
    "X_median_mm_77.shape, y_median_mm_77.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca60745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize class distribution\n",
    "def plot_class_distribution(y, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=pd.Series(y).value_counts().index, y=pd.Series(y).value_counts().values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Number of Instances\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each dataset\n",
    "# plot_class_distribution(y_min_SS_77, \"Class Distribution (Min Instances)\")\n",
    "plot_class_distribution(y_median_mm_77, \"Class Distribution (Mean Instances)\")\n",
    "# plot_class_distribution(y_median_SS_77, \"Class Distribution (Median Instances)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a59bac",
   "metadata": {},
   "source": [
    "## X_test dataset scaling with pipeline scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8697d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_min_SS_77 = pipeline_min_SS_77.named_steps['scale'].transform(X_test_v0)\n",
    "# X_test_mean_mm_77 = pipeline_mean_mm_77.named_steps['scale'].transform(X_test_v0)\n",
    "X_test_median_mm_77 = pipeline_median_mm_77.named_steps['scale'].transform(X_test_v0)\n",
    "\n",
    "# X_test_min_SS_77.shape, X_test_median_SS_77.shape, X_test_median_SS_77.shape\n",
    "X_test_median_mm_77.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d124e0",
   "metadata": {},
   "source": [
    "## Create Validation Set from min-training set of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a validation set (20 of training data)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_median_mm_77, y_median_mm_77, test_size=0.2, random_state=42, stratify=y_median_mm_77)\n",
    "\n",
    "# Check dataset shapes\n",
    "print(f\"Training Data Shape: {X_train_final.shape}, {y_train_final.shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test Data Shape: {X_test_median_mm_77.shape}, {y_test_v0.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0fc0a",
   "metadata": {},
   "source": [
    "## Training and making prediction on resampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad546098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and prediction with min_dataset\n",
    "\n",
    "# Train the model\n",
    "train_time_median, train_mem_used_median, trained_model_mm_77, history = train_model(\n",
    "    model, X_train_final, y_train_final, X_val, y_val, epochs=20, batch_size=32)\n",
    "\n",
    "print(f\"Training time: {train_time_median} seconds, Memory used: {train_mem_used_median} MB\")\n",
    "\n",
    "# Make predictions\n",
    "predict_time_median, predict_mem_used_median, y_pred_median_mm_77 = make_predictions(trained_model_mm_77, X_test_median_mm_77)\n",
    "\n",
    "print(f\"Prediction time: {predict_time_median} seconds, Memory used: {predict_mem_used_median} MB\")\n",
    "\n",
    "# Generate Classification Report\n",
    "report_median_mm_77, confusion_matrix_median_mm_77 = make_report(\n",
    "    y_test_v0, y_pred_median_ss_77, trained_model_mm_77, 'MLP_model_median_mm_77', \n",
    "    train_time_median, train_mem_used_median, predict_time_median, predict_mem_used_median)\n",
    "\n",
    "print(report_median_mm_77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaeb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract history details\n",
    "train_loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "train_acc = history['train_acc']\n",
    "val_acc = history['val_acc']\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "\n",
    "# Convert numerical predictions back to original labels\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_v0)  # Convert y_test to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_median_mm_77)  # Convert predictions to original labels\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)  # Rotate labels if they are long\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa35e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsf-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
